{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-andreas.fritz.box:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Test App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a1f78e490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Spark Test App')\\\n",
    "    .config('spark.jars', 'jars/postgresql-42.2.14.jar')\\\n",
    "    .getOrCreate() \n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Streaming from Socket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in sentences sent by a socket stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming data is simulated with natcat in the console. Data is sent to localhost 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = \"9999\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stream from socket\n",
    "lines = spark.readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', host)\\\n",
    "    .option('port', port)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate word counts\n",
    "counts = lines.select(F.explode(F.split(\"value\",\" \")).alias(\"word\"))\\\n",
    "            .groupby(\"word\")\\\n",
    "            .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9c59ed72cc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write stream into in-memory table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1 second'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "# Write stream into in-memory table\n",
    "count = counts.writeStream\\\n",
    "    .queryName('words')\\\n",
    "    .trigger(processingTime='1 second')\\\n",
    "    .outputMode('complete')\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send some text by natcat from the console: nc -l 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|World|    1|\n",
      "|  Man|    2|\n",
      "|Super|    1|\n",
      "|Hello|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"words\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of active streams\n",
    "[s.name for s in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop all streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Streaming from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming wage data csv files added to folder \"streaming_drop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare source files\n",
    "df = pd.read_csv(\"Wage.csv\")\n",
    "df = df[[\"year\", \"age\", \"sex\", \"education\", \"health\", \"wage\"]]\n",
    "for k in range(10):\n",
    "    df_part=df.iloc[k*50:k*50+50,:]\n",
    "    df_part.to_csv(f\"streaming_source/wage{k}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema for ingestion\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add(\"year\", T.IntegerType())\n",
    "        .add(\"age\", T.IntegerType())\n",
    "        .add(\"sex\", T.StringType())\n",
    "        .add(\"education\", T.StringType())\n",
    "        .add(\"health\", T.StringType())\n",
    "        .add(\"wage\", T.FloatType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Stream from csv files in drop location\n",
    "dfStreamFiles = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .csv(\"streaming_drop\", schema=schema, header=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stream ready\n",
    "dfStreamFiles.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write arriving files to table with append mode\n",
    "query = dfStreamFiles.writeStream\\\n",
    "    .queryName('wage_data')\\\n",
    "    .outputMode('append')\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------+------------------+--------------+---------+\n",
      "|year|age|    sex|         education|        health|     wage|\n",
      "+----+---+-------+------------------+--------------+---------+\n",
      "|2005| 49|1. Male|5. Advanced Degree|2. >=Very Good| 277.6014|\n",
      "|2004| 43|1. Male|   4. College Grad|2. >=Very Good|272.29477|\n",
      "|2003| 60|1. Male|   4. College Grad|2. >=Very Good| 268.2663|\n",
      "|2009| 35|1. Male|   4. College Grad|2. >=Very Good| 267.9011|\n",
      "|2006| 50|1. Male|5. Advanced Degree|2. >=Very Good|212.84235|\n",
      "|2006| 59|1. Male|5. Advanced Degree|     1. <=Good|200.54326|\n",
      "|2005| 57|1. Male|5. Advanced Degree|2. >=Very Good|200.54326|\n",
      "|2003| 38|1. Male|   4. College Grad|2. >=Very Good|200.54326|\n",
      "|2006| 45|1. Male|   4. College Grad|2. >=Very Good|200.54326|\n",
      "|2003| 37|1. Male|   4. College Grad|2. >=Very Good|200.54326|\n",
      "+----+---+-------+------------------+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 rows by wage\n",
    "dfWage = spark.table(\"wage_data\")\n",
    "dfWage.sort(F.desc(\"wage\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of active streams\n",
    "[s.name for s in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If aggregations are used output mode has to be set to \"complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryAgg = (\n",
    "    dfStreamFiles\n",
    "    .groupby(\"year\")\n",
    "    .avg(\"wage\")\n",
    "    .withColumnRenamed(\"avg(wage)\", \"wage\")\n",
    "    .writeStream\n",
    "    .queryName('wage_by_age')\n",
    "    .outputMode('complete')\n",
    "    .format(\"memory\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWageAgg = spark.table(\"wage_by_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|              wage|\n",
      "+----+------------------+\n",
      "|2003|112.57746773797112|\n",
      "|2007|116.98974895477295|\n",
      "|2006| 115.4145488194057|\n",
      "|2004|103.77844826834543|\n",
      "|2009|123.48056060791015|\n",
      "|2005|107.01629535968488|\n",
      "|2008|118.36346608942205|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWageAgg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryAgg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Streaming into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming wage data csv files added to folder \"streaming_drop\" into local postgres db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema for ingestion\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add(\"year\", T.IntegerType())\n",
    "        .add(\"age\", T.IntegerType())\n",
    "        .add(\"sex\", T.StringType())\n",
    "        .add(\"education\", T.StringType())\n",
    "        .add(\"health\", T.StringType())\n",
    "        .add(\"wage\", T.FloatType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Stream from csv files in drop location\n",
    "dfStreamFiles = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .csv(\"streaming_drop\", schema=schema, header=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stream ready\n",
    "dfStreamFiles.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postgres Connection\n",
    "dbURL = \"jdbc:postgresql://localhost/andreas?user=andreas&password=montana\"\n",
    "jdbcConf = {\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average wage by sex and education and stream into db in batches with \"foreachBatch\"\n",
    "queryAgg = (\n",
    "    dfStreamFiles\n",
    "    .groupby(\"sex\", \"education\")\n",
    "    .avg(\"wage\")\n",
    "    .withColumnRenamed(\"avg(wage)\", \"wage\")\n",
    "    .writeStream\n",
    "    .outputMode('update')\n",
    "    .foreachBatch(lambda df, batchID: {\n",
    "        df.write.jdbc(url=dbURL, table=\"test\", properties=jdbcConf, mode=\"overwrite\")\n",
    "    })\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of active streams\n",
    "[s.name for s in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryAgg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Streaming with Window Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example: Count number of records with wage>100.000 within certain time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Schema for ingestion\n",
    "schema = (\n",
    "    T.StructType()\n",
    "        .add(\"year\", T.IntegerType())\n",
    "        .add(\"age\", T.IntegerType())\n",
    "        .add(\"sex\", T.StringType())\n",
    "        .add(\"education\", T.StringType())\n",
    "        .add(\"health\", T.StringType())\n",
    "        .add(\"wage\", T.FloatType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Stream from csv files in drop location\n",
    "dfStream = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .csv(\"streaming_drop\", schema=schema, header=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStream.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStream = (\n",
    "    dfStream\n",
    "    .withColumn(\"timestamp\", F.current_timestamp())\n",
    "    .where(\"wage>100\")\n",
    "    .withWatermark(\"timestamp\", \"20 seconds\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count in Windows of 15 sec\n",
    "windowedCounts = (\n",
    "    dfStream\n",
    "    .groupBy(F.window(\"timestamp\", \"10 seconds\"))\n",
    "    .count()\n",
    "    .withColumn(\"window\", F.expr(\"cast(window as string)\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postgres Connection\n",
    "dbURL = \"jdbc:postgresql://localhost/andreas?user=andreas&password=montana\"\n",
    "jdbcConf = {\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream into database\n",
    "queryAgg = (\n",
    "    windowedCounts\n",
    "    .writeStream\n",
    "    .queryName('counts')\n",
    "    .option(\"checkpointLocation\",\"checkpoint\") \n",
    "    .outputMode('update')\n",
    "    .foreachBatch(lambda df, batchID: {\n",
    "        df.write.jdbc(url=dbURL, table=\"test2\", properties=jdbcConf, mode=\"append\" )\n",
    "    })\n",
    "    .start()\n",
    ")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of active streams\n",
    "[s.name for s in spark.streams.active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop all streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
